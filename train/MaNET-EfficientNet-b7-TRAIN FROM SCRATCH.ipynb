{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72feeef9",
   "metadata": {},
   "source": [
    "# MAnet... starts a fresh log when training\n",
    "Notebook is setup specifically to start training MAnet from scratch without overwriting any previous entries.\n",
    "\n",
    "A fresh log will be created containing the training progress (train loss, Validation loss, saved epochs.. etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71483c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import copy\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "from torch import cuda\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as tf\n",
    "import segmentation_models_pytorch as smp\n",
    "from segmentation_models_pytorch.metrics.functional import accuracy as acc  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17120826",
   "metadata": {},
   "source": [
    "### Get data sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aabb6a7",
   "metadata": {},
   "source": [
    "TrainFolder should be the folder where the images and labels subfolder are stored\n",
    "if a label is absent for a tile, an empty one is used as default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b5d115",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TrainFolder=\"./Data/trainData/A1234/\"\n",
    "ListImages=os.listdir(os.path.join(TrainFolder, \"images\")) # Create list of images\n",
    "\n",
    "ValidFolder=\"./Data/trainData/Arundo4/\"  #Used for validation and evaluation after training\n",
    "vListImages=os.listdir(os.path.join(ValidFolder, \"images\")) # Create list of validation images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8daef3d",
   "metadata": {},
   "source": [
    "### necessary parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0140e20c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "width=height=256 # image width and height  ( USED to generate a default empty label)\n",
    "batch_size=24 #acceptable sizes on a 24GB GPU:  48 for 256x256 tiles and 16 for 512x512 tiles\n",
    "epochs=200  #looping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180073a9",
   "metadata": {},
   "source": [
    "## declaration of necessary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e886bf8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Data Transformation\n",
    "\n",
    "tensorise=tf.ToTensor()\n",
    "\n",
    "def AdaptMask(Lbl):   #function to adapt mask to Tensor\n",
    "    Lbl=Lbl.astype(np.float32)\n",
    "    Lbl=Lbl/10\n",
    "    Lbl=Lbl.astype(int)\n",
    "    Lbl=tensorise(Lbl)\n",
    "    return Lbl\n",
    "    \n",
    "\n",
    "transformImg= tf.Compose([tf.ToPILImage(),tf.ToTensor(),tf.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ]) #function to adapt image\n",
    "# Normalize parameters are suggested by PyTorch documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3791a763",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#_____These are required if training on batches of Random images instead of all the trainset equally______\n",
    "\n",
    "\n",
    "def ReadRandomImage(): # Used if training on randomly selected images\n",
    "    idx=np.random.randint(0,len(ListImages)) # Select random image\n",
    "    Img=cv2.imread(os.path.join(TrainFolder, \"images\", ListImages[idx]), cv2.IMREAD_COLOR)[:,:,0:3]\n",
    "    Img=transformImg(Img)\n",
    "    \n",
    "    if Path(os.path.join(TrainFolder, \"labels\", ListImages[idx])).is_file():\n",
    "            Lbl=cv2.imread(os.path.join(TrainFolder, \"labels\", ListImages[idx]), cv2.COLOR_GRAY2BGR ) \n",
    "            Lbl=AdaptMask(Lbl)\n",
    "    else: \n",
    "            Lbl=torch.zeros(width, height,dtype=torch.int32)\n",
    "   \n",
    "    return Img,Lbl\n",
    "\n",
    "\n",
    "\n",
    "def LoadBatch(): # Load batch of Random images\n",
    "    images = torch.zeros([batch_size,3,height,width])\n",
    "    labels = torch.zeros([batch_size, height, width])\n",
    "    for i in range(batch_size):\n",
    "        images[i],labels[i]=ReadRandomImage()\n",
    "\n",
    "    return images, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52525c57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#  This used to load the batches of size BatchSize from the selected folder\n",
    "\n",
    "def LoadNext(batchNum,batchSize,folder):\n",
    "    ListOfImages=os.listdir(os.path.join(folder, \"images\"))\n",
    "    images = torch.zeros([batchSize,3,height,width])\n",
    "    labels = torch.zeros([batchSize, height, width])\n",
    "    for item in range(batchSize):\n",
    "        idx=(batchNum*batchSize)+item \n",
    "        #print (\"idx:\",idx, \"  path:\", os.path.join(folder, \"labels\", ListOfImages[idx]) )\n",
    "        Img=cv2.imread(os.path.join(folder, \"images\", ListOfImages[idx]), cv2.IMREAD_COLOR)[:,:,0:3]\n",
    "        Img=transformImg(Img)\n",
    "        \n",
    "        # now we check if the label exists.  We read it ELSE generate blank tensor\n",
    "        if Path(os.path.join(folder, \"labels\", ListImages[idx])).is_file():\n",
    "            Lbl=cv2.imread(os.path.join(folder, \"labels\", ListOfImages[idx]), cv2.COLOR_GRAY2BGR )#[:,:,0:3]\n",
    "            Lbl=AdaptMask(Lbl)\n",
    "        else: \n",
    "            Lbl=torch.zeros(width, height,dtype=torch.int32)\n",
    "        \n",
    "        \n",
    "        images[item]=Img\n",
    "        labels[item]=Lbl\n",
    "            \n",
    "    return images,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf135e3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def learn(imgs,lbls):\n",
    "    \n",
    "    images=torch.autograd.Variable(imgs,requires_grad=False).to(device) # Load image\n",
    "    labels = torch.autograd.Variable(lbls, requires_grad=False).to(device) # Load labels\n",
    "    Pred=Net(images)#['out'] # make prediction\n",
    "    Net.zero_grad()\n",
    "    criterion = torch.nn.CrossEntropyLoss() # Set loss function\n",
    "    Loss=criterion(Pred,labels.long()) # Calculate cross entropy loss\n",
    "    Loss.backward() # Backpropogate loss\n",
    "    this_loss=Loss.data.cpu().numpy()\n",
    "    optimizer.step() #not used see if necessary\n",
    "    return this_loss\n",
    "\n",
    "\n",
    "def validate(imgs,lbls):\n",
    "    \n",
    "    images=torch.autograd.Variable(imgs,requires_grad=False).to(device) # Load image\n",
    "    labels = torch.autograd.Variable(lbls, requires_grad=False).to(device) # Load labels\n",
    "    Pred=Net(images)#['out'] # make prediction\n",
    "    Net.zero_grad()\n",
    "    criterion = torch.nn.CrossEntropyLoss() # Set loss function\n",
    "    Loss=criterion(Pred,labels.long()) # Calculate cross entropy loss\n",
    "    this_loss=Loss.data.cpu().numpy()\n",
    "    \n",
    "    tp, fp, fn, tn = smp.metrics.get_stats(Pred[0:batch_size,1], lbls.long().to(device), mode='binary', threshold=0.5)\n",
    "    accuracy = acc(tp, fp, fn, tn, reduction=\"micro\")\n",
    "    \n",
    "    return this_loss , float(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3591efe7",
   "metadata": {},
   "source": [
    "## Setup the model and prepare for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aafc352",
   "metadata": {},
   "source": [
    "classsegmentation_models_pytorch.MAnet(encoder_name='resnet34', encoder_depth=5, encoder_weights='imagenet', decoder_use_batchnorm=True, decoder_channels=(256, 128, 64, 32, 16), decoder_pab_channels=64, in_channels=3, classes=1, activation=None, aux_params=None)\n",
    "\n",
    "More Models: https://smp.readthedocs.io/en/stable/models.html \n",
    "Backbones to choose from:  https://smp.readthedocs.io/en/stable/encoders.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba2a99b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = smp.MAnet(\n",
    "    encoder_name=\"efficientnet-b7\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "    encoder_weights=\"imagenet\",     # use None or `imagenet` pre-trained weights for encoder initialization\n",
    "    in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "    classes=2,                      # model output channels (number of classes in your dataset, add +1 for background)\n",
    "    # activation='softmax',  #deprecated for some models.  Last activation is self(x)\n",
    ")\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(\"device: \", device)\n",
    "\n",
    "Net = model # Load net\n",
    "Net=Net.to(device)\n",
    "\n",
    "#model_naming_title=\"MaNet-ENb7\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bc5617",
   "metadata": {},
   "source": [
    "# Train a model FROM SCRATCH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985f41fb",
   "metadata": {},
   "source": [
    "This will create a separate folder here checkpoints are stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821a5f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_naming_title=\"MaNet-ENb7\"\n",
    "t=datetime.now()\n",
    "DateTime =str(t.hour)+str(t.minute)+\"-\"+str(t.day)+\"-\"+str(t.month)+\"-\"+str(t.year)\n",
    "\n",
    "path=model_naming_title+\"-\"+DateTime\n",
    "del t\n",
    "del DateTime\n",
    "if not os.path.exists(path): os.makedirs(path)\n",
    "\n",
    "log_path='LOG for '+model_naming_title+'.csv'\n",
    "\n",
    "if not os.path.exists(log_path):\n",
    "    log_path2=\"./\"+path+'/LOG for '+path+'.csv'\n",
    "    log_titles=['Epoch','Train-Loss','Val-Loss', 'Acc', 'Learn-Rate','Session','CheckPoint']\n",
    "    log_DB=pd.DataFrame( columns=log_titles)\n",
    "    log_DB2=pd.DataFrame( columns=log_titles)\n",
    "    model_naming_title=\"-\"+model_naming_title+\".torch\"\n",
    "\n",
    "    print(\" Folder for checkpoints:'\",path,\"' was created\")\n",
    "    print(\" Training Log File:'\",log_path,\"' will be created... Starting training from scratch\")\n",
    "    Learning_Rate=1e-04\n",
    "    best_loss=1\n",
    "    EpochsStartFrom=0  #in case training is restarted from a previously saved epoch, this continues the sequence\n",
    "    # and prevents over-writing models and logs in the loss database \n",
    "\n",
    "    optimizer=torch.optim.Adam(params=Net.parameters(),lr=Learning_Rate) # Create adam optimizer\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n",
    "\n",
    "    unbatched=len(ListImages)%batch_size\n",
    "    batch_counts=round((len(ListImages)-unbatched)/batch_size)\n",
    "    vunbatched=len(vListImages)%batch_size\n",
    "    vbatch_counts=round((len(vListImages)-vunbatched)/batch_size)\n",
    "    valid_loss=0\n",
    "    ValACC=0\n",
    "else: \n",
    "    print(\"Log file for \",model_naming_title, \" already present as: \", log_path)\n",
    "    print(\"Training will not start, to prevent overwriting\")\n",
    "    print(\" \")\n",
    "    print(\"If you really want to start from scratch, please move the existent log file\")\n",
    "    print(\"If you want to continue training please use the Notebook: MaNET-EfficientNet-b7-TRAIN FROM CHECKPOINT\")\n",
    "\n",
    "#_________________________PREPERATION DONE_________________#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e009df5",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f536863d",
   "metadata": {},
   "source": [
    "# Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d970b0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#_________________________TRAINING LOOP STARTS FROM HERE_________________#\n",
    "for itr in range(epochs): # Training loop\n",
    "    start_time= datetime.now()\n",
    "    train_loss=0\n",
    "    runs=batch_counts   #if this causes the training to halt see previous cell's output\n",
    "    vruns=vbatch_counts\n",
    "    \n",
    "\n",
    "    for batchNum in tqdm(range(batch_counts)):\n",
    "        images,labels=LoadNext(batchNum,batch_size,TrainFolder)\n",
    "        train_loss=train_loss+learn(images, labels)\n",
    "        del images\n",
    "        del labels\n",
    "        gc.collect()\n",
    "        cuda.empty_cache()\n",
    "       \n",
    "    if unbatched>0:\n",
    "        images,labels=LoadNext(batch_counts+1,unbatched,TrainFolder)\n",
    "        train_loss=train_loss+learn(images, labels)\n",
    "        runs=batch_counts+1\n",
    "        del images\n",
    "        del labels\n",
    "        gc.collect()\n",
    "        cuda.empty_cache()\n",
    "    \n",
    "    #uncomment if you want to train on a random batch too\n",
    "    \"\"\"\n",
    "    images,labels=LoadBatch() \n",
    "    train_loss+=learn(images, labels)\n",
    "    runs=batch_counts+1\n",
    "    \"\"\"\n",
    "\n",
    "    train_loss=train_loss/(runs) # +1) #averages the loss on all batches\n",
    "    scheduler.step(train_loss)\n",
    "    \n",
    "    #BEGIN Validation \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        valid_loss=0\n",
    "        ValACC=0\n",
    "        \n",
    "        for vbatchNum in tqdm(range(vbatch_counts)):\n",
    "            images,labels=LoadNext(vbatchNum,batch_size,ValidFolder)\n",
    "            newVloss,batch_accuracy=validate(images, labels)\n",
    "            del images\n",
    "            del labels\n",
    "            gc.collect()\n",
    "            cuda.empty_cache()\n",
    "            valid_loss=valid_loss+newVloss\n",
    "            ValACC=ValACC+batch_accuracy\n",
    "       \n",
    "        if vunbatched>0:\n",
    "            images,labels=LoadNext(vbatch_counts+1,vunbatched,ValidFolder)\n",
    "            newVloss,batch_accuracy=validate(images, labels)\n",
    "            del images\n",
    "            del labels\n",
    "            gc.collect()\n",
    "            cuda.empty_cache()\n",
    "            valid_loss=valid_loss+newVloss\n",
    "            ValACC=ValACC+batch_accuracy\n",
    "            vruns=vbatch_counts+1\n",
    "        \n",
    "        valid_loss=valid_loss/(vruns) #averages the loss on all batches\n",
    "        ValACC=ValACC/(vruns)\n",
    "            \n",
    "    #END   Validation \n",
    "     \n",
    "    duration=datetime.now()-start_time\n",
    "        \n",
    "    if train_loss<=best_loss:\n",
    "        best_loss=copy.deepcopy(train_loss)\n",
    "        t=datetime.now()\n",
    "        checkpoint=path+\"/\"+str(itr+EpochsStartFrom)+\"-\"+str(t.hour)+str(t.minute)+model_naming_title\n",
    "        print(\"Saving Model: \", \"./\"+checkpoint)\n",
    "        torch.save(Net.state_dict(),\"./\"+checkpoint)\n",
    "    else:\n",
    "        if itr!=epochs-1:  checkpoint=\"not saved\"\n",
    "        else:\n",
    "            checkpoint=path+\"/\"+str(itr+EpochsStartFrom)+\"-LAST EPOCH-\"+model_naming_title\n",
    "            torch.save(Net.state_dict(),\"./\"+checkpoint)\n",
    "            print(\" Saved LAST EPOCH: ./\",checkpoint)\n",
    "        \n",
    "    print(itr+EpochsStartFrom,\"=> TrainLoss=\",train_loss,\"  ValLoss=\", valid_loss,  \"ACC=\",ValACC, \"lr:\", scheduler.state_dict()[\"_last_lr\"][0], \" Time:\", duration.seconds)\n",
    "    new_log_entry=pd.DataFrame([[itr+EpochsStartFrom, train_loss, valid_loss,ValACC, float(scheduler.state_dict()[\"_last_lr\"][0]),path,checkpoint]], columns=log_titles)\n",
    "    log_DB=pd.concat([log_DB, new_log_entry])\n",
    "    log_DB.to_csv(log_path, sep=\",\")\n",
    "    log_DB2=pd.concat([log_DB2, new_log_entry])\n",
    "    log_DB2.to_csv(log_path2, sep=\",\")\n",
    "\n",
    "#_________________________TRAINING LOOP ENDS HERE_________________#\n",
    "        \n",
    "print(\"____FINISHED Training______\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffdc3a2",
   "metadata": {},
   "source": [
    "### Plot training loss graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ee2fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "log_DB=pd.read_csv(log_path, sep=\",\")\n",
    "\n",
    "xAxis=log_DB['Epoch']\n",
    "yAxis=log_DB['Train-Loss']\n",
    "yAxis2=log_DB['Valid-Loss']\n",
    "\n",
    "plt.plot(xAxis,yAxis,xAxis,yAxis2)\n",
    "\n",
    "plt.title('Loss Chart')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47811b57",
   "metadata": {},
   "source": [
    "# Evaluate accuracy over all of the training data (tiles)\n",
    "Uses Confusion Matrix function on each tile/label combo "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8b71dd",
   "metadata": {},
   "source": [
    "### Confusion Matrix function for pixel segmentation (for one class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ba1fe6",
   "metadata": {},
   "source": [
    "segmentation_models_pytorch.metrics.functional.accuracy(tp, fp, fn, tn, reduction=None, class_weights=None, zero_division=1.0)\n",
    "https://smp.readthedocs.io/en/stable/metrics.html#segmentation_models_pytorch.metrics.functional.accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb936bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def bitmapConfMatrix(Output, Target):\n",
    "    tp, fp, fn, tn = smp.metrics.get_stats(Output, Target, mode='multilabel', threshold=0.5)\n",
    "    \n",
    " \n",
    "    TruePs=int(torch.count_nonzero(tp).cpu().detach().numpy())\n",
    "    FalsePs=int(torch.count_nonzero(fp).cpu().detach().numpy())\n",
    "    TrueNs=int(torch.count_nonzero(tn).cpu().detach().numpy())\n",
    "    FalseNs=int(torch.count_nonzero(fn).cpu().detach().numpy())\n",
    "    acc=float(smp.metrics.accuracy(tp, fp, fn, tn, reduction=\"micro\"))\n",
    "\n",
    "    return TruePs,FalsePs,FalseNs,TrueNs, acc\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2dc2e8",
   "metadata": {},
   "source": [
    "### Load Data and Model (if needed, uncomment the following cell & set the modelPath)\n",
    "If this runs right after training, the last state (checkpoint) of the model will be evaluated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4444eeb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ValidFolder=\"./Data/trainData/Arundo4/\"\n",
    "vListImages=os.listdir(os.path.join(ValidFolder, \"images\")) # Create list of validation images\n",
    "\n",
    "modelPath = \"PathToMyBestTrainedModel.torch\"  # Path to trained model\n",
    "\n",
    "Net.load_state_dict(torch.load(modelPath))\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894c7297",
   "metadata": {},
   "source": [
    "### Evaluation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfb848f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ValACC=0\n",
    "tp=0\n",
    "fp=0\n",
    "fn=0\n",
    "tn=0\n",
    "\n",
    "Net.eval()  #  sets network to evaluation model\n",
    "with torch.no_grad():  # tells the Net not to perform gradient descent (since we are only evaluating)\n",
    "    for i in range(len(vListImages)):\n",
    "\n",
    "        idx=i\n",
    "\n",
    "        Img=cv2.imread(os.path.join(ValidFolder, \"images\", vListImages[idx]), cv2.IMREAD_COLOR)#[:,:,0:3]\n",
    "        Img=transformImg(Img)\n",
    "        image=torch.autograd.Variable(Img, requires_grad=False).to(device).unsqueeze(0) # Load image\n",
    "        \n",
    "        Lbl=cv2.imread(os.path.join(ValidFolder, \"labels\", vListImages[idx]), cv2.COLOR_GRAY2BGR )#[:,:,0:3]\n",
    "        if type(Lbl)==type(None): Lbl=np.zeros((width, height), dtype=np.int8)\n",
    "        Lbl=Lbl/10\n",
    "        Lbl=Lbl.astype(np.int8)\n",
    "        Target= torch.from_numpy(Lbl).to(device)\n",
    "        \n",
    "        Pred=Net(image)\n",
    "        Output=Pred[0][1]\n",
    "       \n",
    "        a,b,c,d,e=bitmapConfMatrix(Output, Target)\n",
    "        tp=tp+a\n",
    "        fp=fp+b\n",
    "        fn=fn+c\n",
    "        tn=tn+d\n",
    "        ValACC=ValACC+e\n",
    "        print(\"TP:\",a ,\" FP:\",b,\" FN:\",c,\" TN:\",d,\" Acc:\", e)\n",
    "        \n",
    "\n",
    "ValACC=ValACC/len(vListImages)\n",
    "print(\"FINISHED\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e40e9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Generate Result\n",
    "\n",
    "print(\"TP:\",tp,\"  FP:\",fp)\n",
    "print(\"FN:\",fn,\"  TN:\",tn)\n",
    "print(\"\\nTP% :\",tp/(tp+fp)  )\n",
    "print(\"TN% :\",tn/(fn+tn))\n",
    "\n",
    "\n",
    "print(\"\\nAccuracy:\", ValACC*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788fda6f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
