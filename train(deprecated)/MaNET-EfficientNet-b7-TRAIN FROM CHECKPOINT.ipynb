{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70ff856a",
   "metadata": {},
   "source": [
    "# Continue Training More Epochs from last one saved\n",
    "Notebook is setup specifically for MAnet  to continue training from a checkpoint.\n",
    "\n",
    "It also continues updating the same log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff15dbdc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import copy\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "from torch import cuda\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as tf\n",
    "import segmentation_models_pytorch as smp\n",
    "from segmentation_models_pytorch.metrics.functional import accuracy as acc  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e32ff4",
   "metadata": {},
   "source": [
    "### Get data sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606b015f",
   "metadata": {},
   "source": [
    "TrainFolder should be the folder where the images and labels subfolder are stored\n",
    "if a label is absent for a tile, an empty one is used as default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22f5199f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TrainFolder=\"./Data/trainData/A1234/\"\n",
    "ListImages=os.listdir(os.path.join(TrainFolder, \"images\")) # Create list of images\n",
    "\n",
    "ValidFolder=\"./Data/trainData/Arundo4/\"  #Used for validation and evaluation after training\n",
    "vListImages=os.listdir(os.path.join(ValidFolder, \"images\")) # Create list of validation images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37db1442",
   "metadata": {},
   "source": [
    "### necessary parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab9d5f09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "width=height=256 # image width and height  ( USED to generate a default empty label)\n",
    "batch_size=24 #acceptable sizes on a 24GB GPU:  48 for 256x256 tiles and 16 for 512x512 tiles\n",
    "epochs=200  #looping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af89f222",
   "metadata": {},
   "source": [
    "## declaration of necessary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ed3a778",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Data Transformation\n",
    "\n",
    "tensorise=tf.ToTensor()\n",
    "\n",
    "def AdaptMask(Lbl):   #function to adapt mask to Tensor\n",
    "    Lbl=Lbl.astype(np.float32)\n",
    "    Lbl=Lbl/10\n",
    "    Lbl=Lbl.astype(int)\n",
    "    Lbl=tensorise(Lbl)\n",
    "    return Lbl\n",
    "    \n",
    "\n",
    "transformImg= tf.Compose([tf.ToPILImage(),tf.ToTensor(),tf.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ]) #function to adapt image\n",
    "# Normalize parameters are suggested by PyTorch documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb8db225",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#_____These are required if training on batches of Random images instead of all the trainset equally______\n",
    "\n",
    "\n",
    "def ReadRandomImage(): # Used if training on randomly selected images\n",
    "    idx=np.random.randint(0,len(ListImages)) # Select random image\n",
    "    Img=cv2.imread(os.path.join(TrainFolder, \"images\", ListImages[idx]), cv2.IMREAD_COLOR)[:,:,0:3]\n",
    "    Img=transformImg(Img)\n",
    "    \n",
    "    if Path(os.path.join(TrainFolder, \"labels\", ListImages[idx])).is_file():\n",
    "            Lbl=cv2.imread(os.path.join(TrainFolder, \"labels\", ListImages[idx]), cv2.COLOR_GRAY2BGR ) \n",
    "            Lbl=AdaptMask(Lbl)\n",
    "    else: \n",
    "            Lbl=torch.zeros(width, height,dtype=torch.int32)\n",
    "   \n",
    "    return Img,Lbl\n",
    "\n",
    "\n",
    "\n",
    "def LoadBatch(): # Load batch of Random images\n",
    "    images = torch.zeros([batch_size,3,height,width])\n",
    "    labels = torch.zeros([batch_size, height, width])\n",
    "    for i in range(batch_size):\n",
    "        images[i],labels[i]=ReadRandomImage()\n",
    "\n",
    "    return images, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebd2aee7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#  This used to load the batches of size BatchSize from the selected folder\n",
    "\n",
    "def LoadNext(batchNum,batchSize,folder):\n",
    "    ListOfImages=os.listdir(os.path.join(folder, \"images\"))\n",
    "    images = torch.zeros([batchSize,3,height,width])\n",
    "    labels = torch.zeros([batchSize, height, width])\n",
    "    for item in range(batchSize):\n",
    "        idx=(batchNum*batchSize)+item \n",
    "        #print (\"idx:\",idx, \"  path:\", os.path.join(folder, \"labels\", ListOfImages[idx]) )\n",
    "        Img=cv2.imread(os.path.join(folder, \"images\", ListOfImages[idx]), cv2.IMREAD_COLOR)[:,:,0:3]\n",
    "        Img=transformImg(Img)\n",
    "        \n",
    "        # now we check if the label exists.  We read it ELSE generate blank tensor\n",
    "        if Path(os.path.join(folder, \"labels\", ListImages[idx])).is_file():\n",
    "            Lbl=cv2.imread(os.path.join(folder, \"labels\", ListOfImages[idx]), cv2.COLOR_GRAY2BGR )#[:,:,0:3]\n",
    "            Lbl=AdaptMask(Lbl)\n",
    "        else: \n",
    "            Lbl=torch.zeros(width, height,dtype=torch.int32)\n",
    "        \n",
    "        \n",
    "        images[item]=Img\n",
    "        labels[item]=Lbl\n",
    "            \n",
    "    return images,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c8927c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def learn(imgs,lbls):\n",
    "    \n",
    "    images=torch.autograd.Variable(imgs,requires_grad=False).to(device) # Load image\n",
    "    labels = torch.autograd.Variable(lbls, requires_grad=False).to(device) # Load labels\n",
    "    Pred=Net(images)#['out'] # make prediction\n",
    "    Net.zero_grad()\n",
    "    criterion = torch.nn.CrossEntropyLoss() # Set loss function\n",
    "    Loss=criterion(Pred,labels.long()) # Calculate cross entropy loss\n",
    "    Loss.backward() # Backpropogate loss\n",
    "    this_loss=Loss.data.cpu().numpy()\n",
    "    optimizer.step() #not used see if necessary\n",
    "    return this_loss\n",
    "\n",
    "\n",
    "def validate(imgs,lbls):\n",
    "    \n",
    "    images=torch.autograd.Variable(imgs,requires_grad=False).to(device) # Load image\n",
    "    labels = torch.autograd.Variable(lbls, requires_grad=False).to(device) # Load labels\n",
    "    Pred=Net(images)#['out'] # make prediction\n",
    "    Net.zero_grad()\n",
    "    criterion = torch.nn.CrossEntropyLoss() # Set loss function\n",
    "    Loss=criterion(Pred,labels.long()) # Calculate cross entropy loss\n",
    "    this_loss=Loss.data.cpu().numpy()\n",
    "    \n",
    "    tp, fp, fn, tn = smp.metrics.get_stats(Pred[0:batch_size,1], lbls.long().to(device), mode='binary', threshold=0.5)\n",
    "    accuracy = acc(tp, fp, fn, tn, reduction=\"micro\")\n",
    "    \n",
    "    return this_loss , float(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59621a5c",
   "metadata": {},
   "source": [
    "## Setup the model and prepare for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed392835",
   "metadata": {},
   "source": [
    "classsegmentation_models_pytorch.MAnet(encoder_name='resnet34', encoder_depth=5, encoder_weights='imagenet', decoder_use_batchnorm=True, decoder_channels=(256, 128, 64, 32, 16), decoder_pab_channels=64, in_channels=3, classes=1, activation=None, aux_params=None)\n",
    "\n",
    "More Models: https://smp.readthedocs.io/en/stable/models.html \n",
    "Backbones to choose from:  https://smp.readthedocs.io/en/stable/encoders.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74905def",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device:  cuda\n"
     ]
    }
   ],
   "source": [
    "model = smp.MAnet(\n",
    "    encoder_name=\"efficientnet-b7\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "    encoder_weights=\"imagenet\",     # use None or `imagenet` pre-trained weights for encoder initialization\n",
    "    in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "    classes=2,                      # model output channels (number of classes in your dataset, add +1 for background)\n",
    "    # activation='softmax',  #deprecated for some models.  Last activation is self(x)\n",
    ")\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(\"device: \", device)\n",
    "\n",
    "Net = model # Load net\n",
    "Net=Net.to(device)\n",
    "\n",
    "#model_naming_title=\"MaNet-ENb7\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e667fc05",
   "metadata": {},
   "source": [
    "# Continue training a model  FROM CHECKPOINT\n",
    "This includes:\n",
    ">using a log to keep track of training progress\n",
    "\n",
    ">continue training from the last checkpoint (number & learning rate detected from log)\n",
    "\n",
    "> Saving the last Epoch in the loop regardless of result (in case you want to continue training from it later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6bcbfc47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A log file for  MaNet-ENb7  was found as:  LOG for MaNet-ENb7.csv\n",
      "Folder for checkpoints:  MaNet-ENb7-1641-15-11-2022  was found\n",
      "Training to continue from checkpoint: ./MaNet-ENb7-1641-15-11-2022/4-1642-MaNet-ENb7.torch\n"
     ]
    }
   ],
   "source": [
    "model_naming_title=\"MaNet-ENb7\"  #MaNet with backbone EfficientNet-b7\n",
    "\n",
    "log_path='LOG for '+model_naming_title+'.csv'\n",
    "log_titles=['Epoch','Train-Loss','Val-Loss', 'Acc', 'Learn-Rate','Session','CheckPoint']\n",
    "log_DB=pd.DataFrame( columns=log_titles)\n",
    "\n",
    "if os.path.exists(log_path):\n",
    "    print(\"A log file for \",model_naming_title,\" was found as: \",log_path)\n",
    "    log_DB=pd.read_csv(log_path, sep=\",\", index_col=0)\n",
    "    path=log_DB.tail(1)['Session']\n",
    "    path=str(path[0])\n",
    "    best_loss=log_DB['Train-Loss'].min() #smallest loss value\n",
    "    LastEpoch=int(log_DB.tail(1)['Epoch'])\n",
    "    LastCheckpoint=log_DB.tail(1)['CheckPoint']\n",
    "    Learning_Rate=float(log_DB.tail(1)['Learn-Rate'])  #the last learning rate logged\n",
    "    EpochsStartFrom=LastEpoch+1\n",
    "    \n",
    "    \n",
    "    if os.path.exists(path):\n",
    "        print(\"Folder for checkpoints: \",path, \" was found\")\n",
    "        checkpoint =\"./\"+LastCheckpoint[0] # Path to trained model\n",
    "        \n",
    "        if os.path.exists(checkpoint):\n",
    "            print(\"Training to continue from checkpoint:\", checkpoint)\n",
    "            \n",
    "        else: \n",
    "            print(\"Last Checkpoint: \",checkpoint, \" was not found.  Training cannot continue\")\n",
    "            #del epochs\n",
    "            del checkpoint\n",
    "            print(\" Please specify a path to a saved checkpoint manually in the next cell\")\n",
    "    else:\n",
    "        print(\"Folder for checkpoints: \",path, \" was not found.  Training cannot continue\")\n",
    "        del epochs\n",
    "        print(\" Please restore the folder and restart this notebook, or start training from scratch in appropriate notebook\")\n",
    "\n",
    "else:\n",
    "    print(\" Training Log File:  '\",log_path,\"'  was not found...  \")\n",
    "    \n",
    "    print(\" Please restore the log file and restart this notebook, or start training from scratch in appropriate notebook\")\n",
    "\n",
    "\n",
    "#_________________________PREPERATION DONE_________________#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c835b89d",
   "metadata": {},
   "source": [
    "### Load Checkpoint manually (if instructed by previous cell)\n",
    "You shouldn't need to do this if all works well.  This is provided only as a contingency in case of power cuts or other interruptions to the training.\n",
    "\n",
    "If no checkpoint has been detected by the previous cell, training won't take place unless you specify a checkpoint.\n",
    "\n",
    "Uncomment the next cell to specify a checkpoint manually.  Otherwise ignore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1621a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint=\"./\"+path+\"/\"+\"COPY CHECKPOINT NAME HERE\"+\".torch\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d42dfda",
   "metadata": {},
   "source": [
    "# Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecfdde9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Net.load_state_dict(torch.load(checkpoint))  #if this gives an error check the training setup in previous 2 cells\n",
    "optimizer=torch.optim.Adam(params=Net.parameters(),lr=Learning_Rate) # Create adam optimizer\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n",
    "            \n",
    "model_naming_title=\"-\"+model_naming_title+\".torch\"\n",
    "log_path2=\"./\"+path+'/LOG for '+path+'.csv'\n",
    "log_DB2=pd.read_csv(log_path2, sep=\",\", index_col=0)\n",
    "            \n",
    "unbatched=len(ListImages)%batch_size\n",
    "batch_counts=round((len(ListImages)-unbatched)/batch_size)\n",
    "vunbatched=len(vListImages)%batch_size\n",
    "vbatch_counts=round((len(vListImages)-vunbatched)/batch_size)\n",
    "\n",
    "valid_loss=0\n",
    "ValACC=0\n",
    "\n",
    "\n",
    "#_________________________TRAINING STARTS FROM HERE_________________#\n",
    "for itr in range(epochs): # Training loop\n",
    "    start_time= datetime.now()\n",
    "    train_loss=0\n",
    "    runs=batch_counts\n",
    "    vruns=vbatch_counts\n",
    "    \n",
    "\n",
    "    for batchNum in tqdm(range(batch_counts)):\n",
    "        images,labels=LoadNext(batchNum,batch_size,TrainFolder)\n",
    "        train_loss=train_loss+learn(images, labels)\n",
    "        del images\n",
    "        del labels\n",
    "        gc.collect()\n",
    "        cuda.empty_cache()\n",
    "       \n",
    "    if unbatched>0:\n",
    "        images,labels=LoadNext(batch_counts+1,unbatched,TrainFolder)\n",
    "        train_loss=train_loss+learn(images, labels)\n",
    "        runs=batch_counts+1\n",
    "        del images\n",
    "        del labels\n",
    "        gc.collect()\n",
    "        cuda.empty_cache()\n",
    "    \n",
    "    #uncomment if you want to train on a random batch too\n",
    "    \"\"\"\n",
    "    images,labels=LoadBatch() \n",
    "    train_loss+=learn(images, labels)\n",
    "    runs=batch_counts+1\n",
    "    \"\"\"\n",
    "\n",
    "    train_loss=train_loss/(runs) # +1) #averages the loss on all batches\n",
    "    scheduler.step(train_loss)\n",
    "    \n",
    "    #BEGIN Validation \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        valid_loss=0\n",
    "        ValACC=0\n",
    "        \n",
    "        for vbatchNum in tqdm(range(vbatch_counts)):\n",
    "            images,labels=LoadNext(vbatchNum,batch_size,ValidFolder)\n",
    "            newVloss,batch_accuracy=validate(images, labels)\n",
    "            del images\n",
    "            del labels\n",
    "            gc.collect()\n",
    "            cuda.empty_cache()\n",
    "            valid_loss=valid_loss+newVloss\n",
    "            ValACC=ValACC+batch_accuracy\n",
    "       \n",
    "        if vunbatched>0:\n",
    "            images,labels=LoadNext(vbatch_counts+1,vunbatched,ValidFolder)\n",
    "            newVloss,batch_accuracy=validate(images, labels)\n",
    "            del images\n",
    "            del labels\n",
    "            gc.collect()\n",
    "            cuda.empty_cache()\n",
    "            valid_loss=valid_loss+newVloss\n",
    "            ValACC=ValACC+batch_accuracy\n",
    "            vruns=vbatch_counts+1\n",
    "        \n",
    "        valid_loss=valid_loss/(vruns) #averages the loss on all batches\n",
    "        ValACC=ValACC/(vruns)\n",
    "            \n",
    "    #END   Validation \n",
    "     \n",
    "    duration=datetime.now()-start_time\n",
    "        \n",
    "    if train_loss<=best_loss:\n",
    "        best_loss=copy.deepcopy(train_loss)\n",
    "        t=datetime.now()\n",
    "        checkpoint=path+\"/\"+str(itr+EpochsStartFrom)+\"-\"+str(t.hour)+str(t.minute)+model_naming_title\n",
    "        print(\"Saving Model: \", \"./\"+checkpoint)\n",
    "        torch.save(Net.state_dict(),\"./\"+checkpoint)\n",
    "    else:\n",
    "        if itr!=epochs-1:  checkpoint=\"not saved\"\n",
    "        else:\n",
    "            checkpoint=path+\"/\"+str(itr+EpochsStartFrom)+\"-LAST EPOCH-\"+model_naming_title\n",
    "            torch.save(Net.state_dict(),\"./\"+checkpoint)\n",
    "            print(\" Saving LAST EPOCH: ./\",checkpoint)\n",
    "        \n",
    "    print(itr+EpochsStartFrom,\"=> TrainLoss=\",train_loss,\"  ValLoss=\", valid_loss,  \"ACC=\",ValACC, \"lr:\", scheduler.state_dict()[\"_last_lr\"][0], \" Time:\", duration.seconds)\n",
    "    new_log_entry=pd.DataFrame([[itr+EpochsStartFrom, train_loss, valid_loss,ValACC, float(scheduler.state_dict()[\"_last_lr\"][0]),path,checkpoint]], columns=log_titles)\n",
    "    log_DB=pd.concat([log_DB, new_log_entry])\n",
    "    log_DB.to_csv(log_path, sep=\",\")\n",
    "    log_DB2=pd.concat([log_DB2, new_log_entry])\n",
    "    log_DB2.to_csv(log_path2, sep=\",\")\n",
    "\n",
    "#_________________________TRAINING LOOP ENDS HERE_________________#\n",
    "        \n",
    "print(\"____FINISHED Training______\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328f89a2",
   "metadata": {},
   "source": [
    "### Plot training and validation loss graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "487bc9a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "log_DB=pd.read_csv(log_path, sep=\",\")\n",
    "\n",
    "xAxis=log_DB['Epoch']\n",
    "yAxis=log_DB['Train-Loss']\n",
    "yAxis2=log_DB['Val-Loss']\n",
    "\n",
    "plt.plot(xAxis,yAxis,xAxis,yAxis2)\n",
    "\n",
    "plt.title('Loss Chart')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5414ae14",
   "metadata": {},
   "source": [
    "# Evaluate accuracy over all of the training data (tiles)\n",
    "Uses Confusion Matrix function on each tile/label combo "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a81965",
   "metadata": {},
   "source": [
    "### Confusion Matrix function for pixel segmentation (for one class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c51fd54",
   "metadata": {},
   "source": [
    "segmentation_models_pytorch.metrics.functional.accuracy(tp, fp, fn, tn, reduction=None, class_weights=None, zero_division=1.0)\n",
    "https://smp.readthedocs.io/en/stable/metrics.html#segmentation_models_pytorch.metrics.functional.accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f749bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def bitmapConfMatrix(Output, Target):\n",
    "    tp, fp, fn, tn = smp.metrics.get_stats(Output, Target, mode='multilabel', threshold=0.5)\n",
    "    \n",
    " \n",
    "    TruePs=int(torch.count_nonzero(tp).cpu().detach().numpy())\n",
    "    FalsePs=int(torch.count_nonzero(fp).cpu().detach().numpy())\n",
    "    TrueNs=int(torch.count_nonzero(tn).cpu().detach().numpy())\n",
    "    FalseNs=int(torch.count_nonzero(fn).cpu().detach().numpy())\n",
    "    acc=float(smp.metrics.accuracy(tp, fp, fn, tn, reduction=\"micro\"))\n",
    "\n",
    "    return TruePs,FalsePs,FalseNs,TrueNs, acc\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563dd90d",
   "metadata": {},
   "source": [
    "### Load Data and Model (if needed, uncomment the following cell & set the modelPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8e9718",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ValidFolder=\"./Data/trainData/Arundo4/\"\n",
    "vListImages=os.listdir(os.path.join(ValidFolder, \"images\")) # Create list of validation images\n",
    "\n",
    "modelPath = \"PathToMyBestTrainedModel.torch\"  # Path to trained model\n",
    "\n",
    "Net.load_state_dict(torch.load(modelPath))\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d58ee7",
   "metadata": {},
   "source": [
    "### Evaluation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a430ff",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ValACC=0\n",
    "tp=0\n",
    "fp=0\n",
    "fn=0\n",
    "tn=0\n",
    "\n",
    "Net.eval()  #  sets network to evaluation model\n",
    "with torch.no_grad():  # tells the Net not to perform gradient descent (since we are only evaluating)\n",
    "    for i in range(len(vListImages)):\n",
    "\n",
    "        idx=i\n",
    "\n",
    "        Img=cv2.imread(os.path.join(ValidFolder, \"images\", vListImages[idx]), cv2.IMREAD_COLOR)#[:,:,0:3]\n",
    "        Img=transformImg(Img)\n",
    "        image=torch.autograd.Variable(Img, requires_grad=False).to(device).unsqueeze(0) # Load image\n",
    "        \n",
    "        Lbl=cv2.imread(os.path.join(ValidFolder, \"labels\", vListImages[idx]), cv2.COLOR_GRAY2BGR )#[:,:,0:3]\n",
    "        if type(Lbl)==type(None): Lbl=np.zeros((width, height), dtype=np.int8)\n",
    "        Lbl=Lbl/10\n",
    "        Lbl=Lbl.astype(np.int8)\n",
    "        Target= torch.from_numpy(Lbl).to(device)\n",
    "        \n",
    "        Pred=Net(image)\n",
    "        Output=Pred[0][1]\n",
    "       \n",
    "        a,b,c,d,e=bitmapConfMatrix(Output, Target)\n",
    "        tp=tp+a\n",
    "        fp=fp+b\n",
    "        fn=fn+c\n",
    "        tn=tn+d\n",
    "        ValACC=ValACC+e\n",
    "        print(\"TP:\",a ,\" FP:\",b,\" FN:\",c,\" TN:\",d,\" Acc:\", e)\n",
    "        \n",
    "\n",
    "ValACC=ValACC/len(vListImages)\n",
    "print(\"FINISHED\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2aa29a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Generate Result\n",
    "\n",
    "print(\"TP:\",tp,\"  FP:\",fp)\n",
    "print(\"FN:\",fn,\"  TN:\",tn)\n",
    "print(\"\\nTP% :\",tp/(tp+fp)  )\n",
    "print(\"TN% :\",tn/(fn+tn))\n",
    "\n",
    "\n",
    "print(\"\\nAccuracy:\", ValACC*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ce3342",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
